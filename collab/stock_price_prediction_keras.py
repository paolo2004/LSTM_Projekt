# -*- coding: utf-8 -*-
"""stock_price_prediction_keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VTpUSYXbbK_kDJnDRvZn24Rgb4OYrtFI

<a href="https://colab.research.google.com/github/paolo2004/LSTM_Projekt/blob/master/stock_price_prediction_keras.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Predict stock prices  with Long short-term memory (LSTM)

This simple example will show you how LSTM models predict time series data. Stock market data is a great choice for this because it's quite regular and widely available via the Internet.

## Install requirements
We install Tensorflow 2.0 with GPU support first
"""

!pip install tensorflow-gpu==2.0.0-alpha0

!pip install pandas-datareader

!apt install graphviz

!pip install pydot pydot-ng

"""## Introduction

LSTMs are very powerful in sequence prediction problems. They can store past information.

## Loading the dataset
I use pandas-datareader to get the historical stock prices from Yahoo! finance. For this example, I get only the historical data till the end of *training_end_data*.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pandas_datareader import data

import yfinance as yf

INTERVAL = '60m'
TICKER = 'AAPL'
WINDOW = 60

TRAIN_START = '2024-01-01'
TRAIN_END = '2024-12-31'

TEST_START  = "2025-01-01"
TEST_END    = "2025-10-30"

stock_data = yf.download(TICKER, TRAIN_START, TRAIN_END
, interval=INTERVAL, progress=False)
df_test  = yf.download(TICKER, TEST_START,  TEST_END,  interval=INTERVAL, progress=False)

# concat to be able to create test inputs that use prior train history
df_all = pd.concat([stock_data, df_test]).sort_index()

"""## FEATURES ENGENEERING

"""

# Keep OHLCV and add indicators (RSI, MACD, EMAs, logs returns)
df = df_all.copy()

# Log returns (close)
df['log_ret'] = np.log(df['Close']).diff()

# EMA(s)
df['ema_10'] = df['Close'].ewm(span=10, adjust=False).mean()
df['ema_20'] = df['Close'].ewm(span=20, adjust=False).mean()

# RSI (14)
delta = df['Close'].diff()
up = delta.clip(lower=0)
down = -1 * delta.clip(upper=0)
roll_up = up.ewm(span=14, adjust=False).mean()
roll_down = down.ewm(span=14, adjust=False).mean()
rs = roll_up / (roll_down + 1e-9)
df['rsi_14'] = 100.0 - (100.0 / (1.0 + rs))

# Volume-based (volume and volume change)
df['vol'] = df['Volume']
df['vol_chg'] = df['Volume'].pct_change().fillna(0)

# forward target: predict next log return
df['target'] = df['log_ret'].shift(-1)   # predict t+1 log return

# drop rows with NaNs (first rows from indicators and last row because target is NaN)
df = df.dropna().copy()

"""## Define Features Columns"""

feature_cols = [
    'Close','Open','High','Low','vol',
    'log_ret','ema_10','ema_20','rsi_14',
    'vol_chg'
]
# final check: ensure columns exist
for c in feature_cols + ['target']:
    if c not in df.columns:
        raise RuntimeError(f"Missing column {c}")

"""## Split (time-based)"""

# Use the provided original indexes to split
train_df = df.loc[TRAIN_START:TRAIN_END]
test_df  = df.loc[TEST_START:TEST_END]

# further split train into train/val
val_split_time = train_df.index[int(len(train_df)*0.8)]
val_df = train_df.loc[val_split_time:]
train_df = train_df.loc[:val_split_time]

print("rows - train/val/test:", len(train_df), len(val_df), len(test_df))

"""## Scale (fit on train only)"""

from sklearn.preprocessing import StandardScaler

# Identify and handle infinite values in the DataFrame before splitting
# Replace inf with NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)
# Drop rows that now contain NaN (which includes previously infinite values)
df.dropna(inplace=True)

scaler_X = StandardScaler()
scaler_y = StandardScaler()

# Re-split the data after handling infinities and NaNs
train_df = df.loc[TRAIN_START:TRAIN_END]
test_df  = df.loc[TEST_START:TEST_END]

# further split train into train/val
val_split_time = train_df.index[int(len(train_df)*0.8)]
val_df = train_df.loc[val_split_time:]
train_df = train_df.loc[:val_split_time]

X_train_raw = train_df[feature_cols].values
X_val_raw   = val_df[feature_cols].values
X_test_raw  = test_df[feature_cols].values

y_train_raw = train_df['target'].values.reshape(-1,1)
y_val_raw   = val_df['target'].values.reshape(-1,1)
y_test_raw  = test_df['target'].values.reshape(-1,1)

scaler_X.fit(X_train_raw)
scaler_y.fit(y_train_raw)

X_train = scaler_X.transform(X_train_raw)
X_val   = scaler_X.transform(X_val_raw)
X_test  = scaler_X.transform(X_test_raw)

y_train = scaler_y.transform(y_train_raw)
y_val   = scaler_y.transform(y_val_raw)
y_test  = scaler_y.transform(y_test_raw)

"""## WINDOWING: create sequences (samples, timesteps, features) ---

"""

def make_sequences(X, y, window):
  Xs, ys = [], []
  for i in range(window, len(X)):
        Xs.append(X[i-window:i])
        ys.append(y[i])
  return np.array(Xs), np.array(ys)
X_train_seq, y_train_seq = make_sequences(X_train, y_train, WINDOW)
X_val_seq,   y_val_seq   = make_sequences(X_val,   y_val,   WINDOW)
X_test_seq,  y_test_seq  = make_sequences(X_test,  y_test,  WINDOW)

print("sequence shapes:", X_train_seq.shape, y_train_seq.shape, X_val_seq.shape, X_test_seq.shape)

from tensorflow.keras import layers, callbacks, models
n_features = X_train_seq.shape[2]

def make_lstm_model(input_shape):
    inp = layers.Input(shape=input_shape)
    x = layers.LSTM(128, return_sequences=True)(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    x = layers.LSTM(64, return_sequences=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(32, activation="relu")(x)
    out = layers.Dense(1, activation="linear")(x)  # regression on next log-return
    model = models.Model(inputs=inp, outputs=out)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')
    return model

model = make_lstm_model((WINDOW, n_features))
model.summary()

# callbacks
es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
ckpt = callbacks.ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True, verbose=0)

"""## Train"""

history = model.fit(
    X_train_seq, y_train_seq,
    validation_data=(X_val_seq, y_val_seq),
    epochs=50,
    batch_size=100,
    callbacks=[es, rlr, ckpt],
    verbose=1
)

"""## EVALUATION: predictions on testset ---"""

from sklearn.metrics import mean_squared_error, accuracy_score
import math
y_pred_scaled = model.predict(X_test_seq)
# inverse transform to get log-return predictions
y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()
y_true = scaler_y.inverse_transform(y_test_seq).flatten()

# metrics: RMSE on log-returns
rmse = math.sqrt(mean_squared_error(y_true, y_pred))
print(f"Test RMSE (log-return): {rmse:.6f}")

# directional accuracy
dir_true = (y_true > 0).astype(int)
dir_pred = (y_pred > 0).astype(int)
dir_acc = accuracy_score(dir_true, dir_pred)
print(f"Directional accuracy: {dir_acc*100:.2f}%")

# quick price-level plotting: reconstruct predicted price path
# we need base price for each predicted point: use closing price aligned with sequences
# for each sequence end index i in test_df, the reference price is the close at time t
test_close = test_df['Close'].values[WINDOW:]  # aligned to y_true
pred_price = test_close * np.exp(y_pred)       # next price ≈ current_close * exp(pred_log_ret)
true_price = test_close * np.exp(y_true)

plt.figure(figsize=(12,5))
plt.plot(test_df.index[WINDOW:], true_price, label='Actual next-close (from true log-ret)')
plt.plot(test_df.index[WINDOW:], pred_price, label='Predicted next-close (from pred log-ret)')
plt.legend()
plt.title(f"{TICKER} predicted vs actual (test set) — next-period price estimate")
plt.show()

"""## SIMPLE STRATEGY BACKTEST (apply sign(pred) to next-period returns) ---"""

strategy_returns = np.sign(y_pred) * y_true  # realized returns (log-return space)
cumulative_strategy = np.exp(np.cumsum(strategy_returns))  # cumulative multiplier
cumulative_hold = np.exp(np.cumsum(y_true))                # buy-and-hold for same periods

plt.figure(figsize=(12,5))
plt.plot(test_df.index[WINDOW:], cumulative_hold, label='Buy & Hold (test)')
plt.plot(test_df.index[WINDOW:], cumulative_strategy, label='Strategy (sign of pred)')
plt.legend()
plt.title('Cumulative returns (test period)')
plt.show()

"""## simple performance stats"""

total_strategy_ret = cumulative_strategy[-1] - 1.0
total_hold_ret = cumulative_hold[-1] - 1.0
sharpe = (np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-9)) * np.sqrt(252*24/1)  # rough annualize for hourly
print(f"Total strategy return (multiplier-1): {total_strategy_ret:.3f}")
print(f"Total buy & hold return (multiplier-1): {total_hold_ret:.3f}")
print(f"Basic Sharpe-like (annualized) for strategy: {sharpe:.3f}")

# save model if you want
model.save('lstm_stock_model.h5')
print("Model saved to lstm_stock_model.h5")

"""Of course, some of the weekdays might be public holidays in which case no price will be available. For this reason, we will fill the missing prices with the latest available prices"""

all_bussinessdays = pd.date_range(start=TRAIN_START, end=TRAIN_END, freq='B')
print(all_bussinessdays)

close_prices = stock_data.reindex(all_bussinessdays)
close_prices = stock_data.fillna(method='ffill')

"""The dataset is now complete and free of missing values. Let's have a look to the data frame summary:

## Feature scaling

LSTMs expect the data in a specific format, usually a 3D tensor. I start by creating data with 60 days and converting it into an array using NumPy. Next, I convert the data into a 3D dimension array with feature_set samples, 60 days and one feature at each step.

Feature tensor with three dimension: features[0] contains the ..., features[1] contains the last 60 hours of values and features [2] contains the  ...

## Create the LSTM network
Let's create a sequenced LSTM network with 50 units. Also the net includes some dropout layers with 0.2 which means that 20% of the neurons will be dropped.
"""

import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (features.shape[1], 1)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units = 50, return_sequences = True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units = 50, return_sequences = True),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units = 50),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(units = 1)
])

print(model.summary())

#tf.keras.utils.plot_model(model, to_file='my_model.png')

# Run tensorboard with the logdir
#import os
#LOG_BASE_DIR = './log'
#os.makedirs(LOG_BASE_DIR, exist_ok=True)

#!ls -l log

"""## Load the Colab TensorBoard extention and start TensorBoard inline"""

#%load_ext tensorboard.notebook
#%tensorboard --logdir {LOG_BASE_DIR}

"""## Define a TensorBoard callback"""

#import datetime
#logdir = os.path.join(LOG_BASE_DIR, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

#from tensorflow.keras.callbacks import TensorBoard

#tbCallBack = TensorBoard(logdir,histogram_freq=1)

"""The model will be compiled and optimize by the adam optimizer and set the loss function as mean_squarred_error"""

model.compile(optimizer = 'adam', loss = 'mean_squared_error')

#import os
#print(os.environ)

#tf.test.gpu_device_name()

#from tensorflow.python.client import device_lib
#device_lib.list_local_devices()

from time import time
start = time()
history = model.fit(features, labels, epochs = 50, batch_size = 100, verbose = 1)
end = time()

print('Total training time {} seconds'.format(end - start))

#  [samples, days, features]
print(features.shape)

testing_start_date = '2025-01-01'
testing_end_date = '2025-10-30'

test_stock_data = yf.download(tickers, testing_start_date, testing_end_date, interval='60m')

test_stock_data.tail()

dates = pd.to_datetime(test_stock_data.index)
dates = pd.to_datetime(dates)

print(dates.shape)

test_stock_data_processed = test_stock_data.iloc[:, 1:2].values

print(test_stock_data_processed.shape)

all_stock_data = pd.concat((stock_data['Close'], test_stock_data['Close']), axis = 0)

inputs = all_stock_data[len(all_stock_data) - len(test_stock_data) - 60:].values
inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)

X_test = []
for i in range(60, 129):
    X_test.append(inputs[i-60:i, 0])

X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
predicted_stock_price = model.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

plt.figure(figsize=(10,6))
plt.plot(test_stock_data_processed, color='blue', label='Actual Apple Stock Price')
plt.plot(predicted_stock_price , color='red', label='Predicted Apple Stock Price')
plt.title('Apple Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Apple Stock Price')
plt.legend()
plt.show()

#inputs = inputs.reshape(-1,1)
#inputs = sc.transform(inputs)


test_inputs = test_stock_data_processed.reshape(-1,1)
test_inputs = sc.transform(test_inputs)


print(test_inputs.shape)

test_features = []
for i in range(60, 291):
    test_features.append(test_inputs[i-60:i, 0])

test_features = np.array(test_features)

test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))
print(test_features.shape)

predicted_stock_price = model.predict(test_features)

predicted_stock_price = sc.inverse_transform(predicted_stock_price)
print(predicted_stock_price.shape)

print(test_stock_data_processed.shape)

plt.figure(figsize=(10,6))
plt.plot(test_stock_data_processed, color='blue', label='Actual Apple Stock Price')
plt.plot(predicted_stock_price , color='red', label='Predicted Apple Stock Price')
plt.title('Apple Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Apple Stock Price')
plt.legend()
plt.show()

"""## Download the model and the weights"""

from google.colab import files

model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)

files.download("model.json")

model.save('weights.h5')
files.download('weights.h5')



